{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4367477,"sourceType":"datasetVersion","datasetId":2568507},{"sourceId":4367592,"sourceType":"datasetVersion","datasetId":2568545}],"dockerImageVersionId":30301,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport random\nimport torchaudio\nimport os\nfrom torch import nn\n\n\nclass GuitarFxDataset(Dataset):\n\n    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.c = 0\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        self.c+=1\n        percentage = round((self.c / len(self.annotations)) * 100, 2)\n        #print(f\"Loading {percentage}%\\r\", end=' ')\n        signal, sr = torchaudio.load(audio_sample_path) # restituisce onda sonora e sample rate\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal,sr) #normalizzo il sample rate\n        signal = self._mix_down_if_necessary(signal) #normalizzo la traccia a mono se non lo è\n        #print(signal.shape)\n        signal = self.transformation(signal) #applico MelSpectrograms\n        #print(f\"Signal after mel: {signal.shape}\")\n        signal = torchaudio.transforms.AmplitudeToDB()(signal) #converto in decibel\n        #print(f\"Signal after decibel: {signal.shape}\")\n        return signal, label\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n\n    def _get_audio_sample_path(self, index):\n        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0]) #iloc è un metodo di pandas e serve per recuperare un dato dal file .csv dati gli indici\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 3]\n    \nclass CNNNetwork(nn.Module):\n\n    def __init__(self, l1_reg=0.0001, l2_reg=0.001):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                padding = 2,\n                stride=1\n            ), \n            nn.ReLU(),     #Rectified Linear Unit\n            nn.MaxPool2d(kernel_size=2),\n            nn.Dropout(0.6)\n        )\n        self.layer4 = nn.Sequential(\n            nn.Linear(70992, 432),#64*81     10368   6912 13056 25024 #?46464 13248 70992\n            nn.ReLU()\n        )\n        self.layer5 = nn.Sequential(\n            nn.Linear(432, 36),\n            nn.ReLU(),\n            nn.Dropout(0.4)\n        )\n       \n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(36, 12) #input size e output size\n        self.softmax = nn.Softmax(dim =1)\n        \n        self.l1_reg = l1_reg\n        self.l2_reg = l2_reg\n        \n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.flatten(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        \n         # Aggiungi la regolarizzazione L1 ai pesi\n        l1_loss = 0\n        for param in self.parameters():\n            l1_loss += torch.norm(param, 1)\n\n        # Aggiungi la regolarizzazione L2 ai pesi\n        l2_loss = 0\n        for param in self.parameters():\n            l2_loss += torch.norm(param, 2)\n        \n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions, self.l1_reg * l1_loss, self.l2_reg * l2_loss\n\nfrom torch.utils.data import WeightedRandomSampler, DataLoader\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport torch.onnx\nBATCH_SIZE = 256\nEPOCHS = 700\nLEARNING_RATE = 0.0001\nANNOTATIONS_FILE = \"../input/guitarfxdataset1/annotations1.csv\"\nAUDIO_DIR = \"../input/guitarfxdataset/audio\"\nSAMPLE_RATE = 44100\n\n\ndef prepareDatasets(dataset, valid_size):\n    train_dataset, validation_dataset = train_test_split(dataset, test_size=valid_size, random_state = 42)\n    return train_dataset, validation_dataset\n\ndef create_data_loader(gfxd, batch_size, sampler):\n    dataloader = DataLoader(gfxd, batch_size= batch_size, shuffle = False, sampler=sampler)\n    return dataloader\ndef weighted_accuracy(logits, labels, class_weights):\n    pred = torch.argmax(logits, dim=1)\n    correct = (pred == labels).float()\n    accuracy = (correct * class_weights[labels]).sum() / class_weights[labels].sum()\n    return accuracy.item()\n\ndef compute_batch_weights(y_batch, num_classes):\n    # compute the class frequencies in the batch\n    class_counts = np.bincount(y_batch, minlength=num_classes)\n    # compute the class weights based on the class frequencies\n    class_weights = 1.0 / class_counts.astype(np.float32)\n    # normalize the class weights so that they sum to num_classes\n    class_weights /= class_weights.sum() * num_classes\n    return torch.tensor(class_weights, dtype=torch.float32).unsqueeze(0)\n\ndef train_one_epoch(model, train_data_loader, validation_loader, optimiser,class_weights,val_class_weights, device):\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    val_criterion = nn.CrossEntropyLoss(weight=val_class_weights)\n    loss = 0.0\n    correct = 0.0\n    val_loss = 0.0\n    val_correct = 0.0\n    i=0\n    model.train()\n    for inputs, targets in train_data_loader:\n        i+=1\n        load_percentage =round((i / len(train_data_loader)) * 100,2)\n        #print(f\"Loading {load_percentage}%\\r\", end=' ')\n        inputs,targets = inputs.to(device), targets.to(device)\n        predictions, l1_loss, l2_loss = model(inputs)\n        loss_weights = class_weights[targets]\n        \n        loss1 =  criterion(predictions, targets)* loss_weights\n        #backpropagate loss and update weights\n        total_loss = loss1.mean() + l1_loss + l2_loss\n        optimiser.zero_grad()\n        total_loss.backward()\n        optimiser.step()\n\n        _, preds = torch.max(predictions, 1)\n        loss += (loss1.mean().item() + l1_loss.item() + l2_loss.item())\n\n        correct += torch.sum(preds == targets.data).item()\n    else: #validation\n        model.eval()\n        with torch.no_grad():\n            total_samples = 0\n            total_weighted_correct = 0\n            for val_input, val_labels in validation_loader:\n                val_input = val_input.to(device)\n                val_labels = val_labels.to(device)\n                val_outputs, _,_=model(val_input)\n                val_loss_weights = val_class_weights[val_labels]\n                \n                val_loss1 = criterion(val_outputs, val_labels)* val_loss_weights\n                _ , val_preds = torch.max(val_outputs, 1)\n                val_loss += val_loss1.mean().item()\n                val_correct += torch.sum(val_preds == val_labels.data).item()\n                total_samples += len(val_labels)\n                total_weighted_correct += (weighted_accuracy(val_outputs, val_labels, val_class_weights) * len(val_labels))\n            epoch_loss = loss*100 / len(train_data_loader.dataset)\n            epoch_f1 =f1_score(targets.data.cpu(), preds.cpu(), average='weighted')# correct *100/ len(train_data_loader.dataset)\n#epoch_acc = torch.sum(correct * weights) / torch.sum(weights)\n            epoch_acc = weighted_accuracy(predictions, targets, class_weights)\n            loss_history.append(epoch_loss)\n            f1_history.append(epoch_f1)\n            correct_history.append(epoch_acc)\n\n            val_epoch_loss = val_loss*100 / len(validation_loader.dataset)\n            val_epoch_f1 = f1_score(val_labels.data.cpu(), val_preds.cpu(), average='weighted') #val_correct*100 / len(validation_loader.dataset)\n            #val_epoch_acc = torch.sum(val_correct * weights) / torch.sum(weights)\n            val_epoch_acc = total_weighted_correct / total_samples\n            val_loss_history.append(val_epoch_loss)\n            val_f1_history.append(val_epoch_f1)\n            val_correct_history.append(val_epoch_acc)\n            print('Training Loss:{:.3f}%, training f1 score:{:.3f}, training accuracy:{:.3f}%'.format(epoch_loss, epoch_f1, epoch_acc*100))\n            print('validation_loss:{:.3f}%, validation f1 score{:.3f}, validation accuracy:{:.3f}%'.format(val_epoch_loss, val_epoch_f1, val_epoch_acc*100))\n            \ndef train (model, train_data_loader, validation_loader, optimiser, class_weights,val_class_weights, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_one_epoch(model, train_data_loader, validation_loader, optimiser,class_weights,val_class_weights, device)\n        print(\"--------------\")\n    print(\"Training is done.\")\n\nif __name__ == \"__main__\":\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using device {device}\")\n    \n#instantiate Dataset and create dataLoader\n    mel_spectograms = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=100,\n    )\n\n    gfxd = GuitarFxDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectograms,\n                            SAMPLE_RATE,\n                            device)\n    \n    #Visualizzo uno o più spettogrammi\n    counter =0\n    while counter <10:\n        random_index = random.randint(0, len(gfxd) - 1)\n        sample_item = gfxd[random_index]\n\n        # Estrai lo spettrogramma di Mel dall'elemento\n        mel_spectrogram = sample_item[0].cpu().numpy().squeeze()\n        print(mel_spectrogram.shape)\n        lbl = sample_item[1]\n\n        # Visualizza lo spettrogramma di Mel\n        print(f\"Il seguente grafico si riferisce a uno spettogramma di label {lbl}\")\n        plt.figure(figsize=(10, 4))\n        plt.imshow(mel_spectrogram, cmap='viridis', origin='lower')\n        plt.title('Spettrogramma di Mel')\n        plt.xlabel('Tempo')\n        plt.ylabel('Frequenza')\n        plt.colorbar(format='%+2.0f dB')\n        plt.show()\n        counter += 1\n    \n    train_dataset, validation_dataset = prepareDatasets(gfxd, 0.33) # 20% of samples for validation\n    \n    \n    label_list = []\n    val_label_list = []\n    for data in train_dataset:\n        label_list.append(data[1])\n    for data in validation_dataset:\n        val_label_list.append(data[1])\n    #class_weights=class_weight.compute_class_weight(class_weight = \"balanced\", classes= np.unique(label_list), y= label_list)\n    #class_weights=torch.tensor(class_weights,dtype=torch.float)\n    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(label_list),y=label_list)\n    class_weights =torch.tensor(class_weights, dtype=torch.float32).to(device)\n    val_class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(val_label_list),y=val_label_list)\n    val_class_weights = torch.tensor(val_class_weights, dtype=torch.float).to(device)\n    \n    weights = [class_weights[label] for label in label_list]\n    sampler = WeightedRandomSampler(weights, len(weights))\n    \n    val_weights = [val_class_weights[label] for label in val_label_list]\n    val_sampler = WeightedRandomSampler(val_weights, len(val_weights))\n    \n    train_dataloader = create_data_loader(train_dataset, BATCH_SIZE, sampler)\n    valid_dataloader = create_data_loader(validation_dataset, BATCH_SIZE, val_sampler)\n    \n    #construct model and assign it to device\n    cnn = CNNNetwork().to(device)\n    print(cnn)\n\n    #initialize loss function + optimiser\n    #loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights,dtype=torch.float).to(device),reduction='mean')\n    optimiser = torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n\n    loss_history = []\n    f1_history =[]\n    correct_history = []\n    val_loss_history = []\n    val_f1_history = []\n    val_correct_history = []\n\n    #train model\n    train(cnn, train_dataloader, valid_dataloader, optimiser,class_weights,val_class_weights, device, EPOCHS)\n    #plot the results\n    plt.plot(loss_history, label='Training Loss')\n    plt.plot(val_loss_history, label='Validation Loss')\n    plt.legend()\n    plt.show()\n    plt.plot(correct_history, label='Training accuracy')\n    plt.plot(val_correct_history, label='Validation accuracy')\n    plt.legend()\n    plt.show()\n    plt.plot(f1_history, label='Training F1 score')\n    plt.plot(val_f1_history, label='Validation F1 score')\n    plt.legend()\n    plt.show()\n\n    torch.save(cnn.state_dict(), \"./feedforwardnet5_3.pth\")\n    \n    from torch.utils.mobile_optimizer import optimize_for_mobile\n    cnn = cnn.to(\"cpu\")\n   \n    cnn.eval() \n\n    # Generate some random noise\n    #1, 100, 173\n    dummy_input = torch.randn(1, 1 , 100, 173, requires_grad=True).to(\"cpu\")\n\n    # Generate the optimized model\n    traced_script_module = torch.jit.trace(cnn, dummy_input)\n    traced_script_module_optimized = optimize_for_mobile(traced_script_module)\n\n    # Save the optimzied model\n    traced_script_module_optimized._save_for_lite_interpreter(\"./model.pt\")\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-23T14:25:42.127806Z","iopub.execute_input":"2023-10-23T14:25:42.128198Z","iopub.status.idle":"2023-10-23T14:31:11.220116Z","shell.execute_reply.started":"2023-10-23T14:25:42.128120Z","shell.execute_reply":"2023-10-23T14:31:11.218761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}